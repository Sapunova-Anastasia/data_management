{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: применяем t-sne\n",
    "\n",
    "Модифицируйте файл *train.py* - добавьте в пайплайн обучения модели сжатие размерности до *n_components=2* с помощью t-sne и обучите модель **в докере** на \"сжатых данных\". Сохраните полученный объект `tsne_tansformer.pkl`, который умеет выполнять это задание.\n",
    "\n",
    "Решением домашки считается модифицированный файл *train.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(filename)-25.25s:%(lineno)-4d | %(message)s'\n",
    "log_filename = \"/www/classifier/data/service.log\"\n",
    "logging.basicConfig(filename=\"log_filename\", level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "# загрузка данных\n",
    "data_source = np.genfromtxt('data/client_segmentation.csv', delimiter=',', skip_header=1)\n",
    "X = data_source[:, :3]\n",
    "y = data_source[:, 3]\n",
    "# обучение модели\n",
    "tsne_transformer = TSNE(n_components=2)\n",
    "X_tsne = tsne_transformer.fit_transform(X)\n",
    "clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "clf.fit(X_tsne, y)\n",
    "\n",
    "# сохраняем модель внутри контейнера в директории /www/classifier\n",
    "with open('tsne_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(tsne_transformer, f)\n",
    "    logging.info('Модель для сжатия данных сохранена в %s' % Path().absolute())\n",
    "with open('data/tsne_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(tsne_transformer, f)\n",
    "\n",
    "with open('clf.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    logging.info('Модель обучена и сохранена в %s' % Path().absolute())\n",
    "with open('data/clf.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "print(f\"Модель обучена! Лог: {log_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: трансформация входных фичей на лету\n",
    "\n",
    "Модифицируйте файл `service.py`: добавьте загрузку объекта для трансформации `tsne_tansformer.pkl` и применяйте её **в докере** для трансформации набора входных фич в сжатые:\n",
    "<pre>\n",
    "[x1, x2, x3] -> [x1_tsne, x2_tsne]\n",
    "</pre>\n",
    "\n",
    "Соответственно, predict надо выполнять на *сжатых* фичах\n",
    "\n",
    "\n",
    "Решением домашки считается модифицированный файл *service.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ВАШ КОД ТУТ --\n",
    "\"\"\"\n",
    "Умеет выполнять классификацию клиентов по трём фичам\n",
    "\n",
    "Запускаем из python3:\n",
    "    python3 service.py\n",
    "Проверяем работоспособность:\n",
    "    curl http://127.0.0.1:5000/\n",
    "\"\"\"\n",
    "import json\n",
    "import http.server\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import socketserver\n",
    "import sys\n",
    "from http import HTTPStatus\n",
    "from re import compile\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# файл, куда посыпятся логи модели\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(filename)-25.25s:%(lineno)-4d | %(message)s'\n",
    "logging.basicConfig(filename=\"/www/classifier/data/service.log\", level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "\n",
    "def parse_params(params) -> dict:\n",
    "    \"\"\"\n",
    "        Выдираем параметры из GET-запроса\n",
    "    \"\"\"\n",
    "    params_list = params.split('&')\n",
    "    params_dict = {}\n",
    "    for param in params_list:\n",
    "        key, value = param.split('=')\n",
    "        params_dict[key] = float(value)\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "class Handler(http.server.SimpleHTTPRequestHandler):\n",
    "    \"\"\"Простой http-сервер\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_response(self) -> dict:\n",
    "        \"\"\"Пример запроса\n",
    "        \n",
    "        http://0.0.0.0:5000/classifier/?x1=1&x2=-2.2&x3=1.05&x4=7&x5=1.7&x6=17\n",
    "        \"\"\"\n",
    "        response = {'ping': 'ok'}\n",
    "        params_parsed = self.path.split('?')\n",
    "        if len(params_parsed) == 2 and self.path.startswith('/classifier'):\n",
    "            params = params_parsed[1]\n",
    "            params_dict = parse_params(params)\n",
    "            response = params_dict\n",
    "            amount_of_elements = len(params_dict)\n",
    "            if (amount_of_elements % 3 == 0) and (amount_of_elements > 3):\n",
    "                user_features = np.zeros(amount_of_elements)\n",
    "                i = 0\n",
    "                for key in params_dict:\n",
    "                    user_features[i] = params_dict[key]\n",
    "                    i = i + 1\n",
    "                user_features = user_features.reshape(int(amount_of_elements / 3), 3)\n",
    "            else:\n",
    "                user_features = np.zeros((2,amount_of_elements))\n",
    "                i = 0\n",
    "                for key in params_dict:\n",
    "                    user_features[0,i] = params_dict[key]\n",
    "                    user_features[1,i] = params_dict[key]\n",
    "                    i = i + 1                   \n",
    "            \n",
    "            transform_user_features = data_compression_model.fit_transform(user_features)\n",
    "            predicted_class = int(classifier_model.predict(transform_user_features)[0])\n",
    "            logging.info('predicted_class %s' % predicted_class)\n",
    "            response.update({'predicted_class': predicted_class})\n",
    "        elif self.path.startswith('/ping/'):\n",
    "            response = {'message': 'pong'}\n",
    "\n",
    "        return response\n",
    "\n",
    "    def do_GET(self):\n",
    "        # заголовки ответа\n",
    "        self.send_response(HTTPStatus.OK)\n",
    "        self.send_header(\"Content-type\", \"application/json\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(self.get_response()).encode())\n",
    "\n",
    "\n",
    "logging.info('Загружаем обученные модели')\n",
    "with open('/www/classifier/tsne_transformer.pkl', 'rb') as f_tsne:\n",
    "    data_compression_model = pickle.load(f_tsne)\n",
    "    logging.info('Модель для сжатия данных загружена: %s' % data_compression_model)\n",
    "\n",
    "with open('/www/classifier/clf.pkl', 'rb') as f:\n",
    "    classifier_model = pickle.load(f)\n",
    "    logging.info('Модель загружена: %s' % classifier_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier_service = socketserver.TCPServer(('', 5000), Handler)\n",
    "    classifier_service.serve_forever()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: Используем Flask\n",
    "\n",
    "Перепишите сервис на использование Flask. Вы можете взять готовый базовый образ с Flask, либо добавить установку в тот контейнер, который есть - это нужно сделать в Dockerfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ВАШ КОД ТУТ --\n",
    "\n",
    "FROM wrwrwr/flask-scipy\n",
    "\n",
    "WORKDIR /www/classifier\n",
    "\n",
    "RUN pip install -U scikit-learn\n",
    "\n",
    "COPY docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh\n",
    "RUN chmod +x /usr/local/bin/docker-entrypoint.sh\n",
    "\n",
    "COPY train.py /www/classifier/train.py\n",
    "COPY service.py /www/classifier/service.py\n",
    "\n",
    "ENTRYPOINT [\"docker-entrypoint.sh\"]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание: строим KNN\n",
    "\n",
    "В реальной жизни KNN-рекомендатель не стоит делать на основе `sklearn.neighbors.NearestNeighbors` - есть готовые реализации, заточенные специально для построения рекомендательных систем. Хорошим примером такой реализации является [пакет implictit](). В рамках домашней работы предлагается разобраться с реализацией KNN-рекомендателя из этой библиотеки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почитайте документацию по модулю `implicit.nearest_neighbours.CosineRecommender`. Обучите KNN-рекомендатель и воспользуйтесь методом `recommend` для построения рекомендаций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ВАШ КОД ТУТ --\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "content_views = pd.read_csv(\n",
    "    'recsys_data/content_views.zip', delimiter=',', header=0, compression='zip',\n",
    "    names = ['user_id', 'content_id', 'view_duration', 'view_ts', 'dt', 'platform'],\n",
    "    dtype = {'user_id': np.uint32, 'content_id': np.uint16, 'view_duration': np.uint16},\n",
    "    parse_dates = [3, 4]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_description = pd.read_csv(\n",
    "    'recsys_data/content_description.zip', delimiter=',', header=0, compression='zip',\n",
    "    names = ['content_id', 'origin_country', 'release_date', 'kinopoisk_rating', 'compilation_id', 'genre'],\n",
    "    dtype = {'content_id': np.uint16},\n",
    "    parse_dates = [2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# кодируем индексы пользователей\n",
    "user_encoder = LabelEncoder()\n",
    "user_encoder.fit(content_views.user_id)\n",
    "\n",
    "# ереиндексация контента\n",
    "content_views = content_views.assign(\n",
    "    user_index = user_encoder.transform(content_views.user_id)\n",
    ")\n",
    "\n",
    "# кодируем индексы контента\n",
    "item_encoder = LabelEncoder()\n",
    "item_encoder.fit(content_views.content_id)\n",
    "\n",
    "# нова переиндексация\n",
    "content_views = content_views.assign(\n",
    "    item_index = item_encoder.transform(content_views.content_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "num_users = content_views.user_index.max() + 1\n",
    "num_items = content_views.item_index.max() + 1\n",
    "num_interactions = content_views.shape[0]\n",
    "\n",
    "user_item = csr_matrix(\n",
    "    (np.ones(num_interactions),(content_views.item_index.values, content_views.user_index.values)),\n",
    "    shape=(num_items, num_users)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 27012/27012 [00:01<00:00, 15962.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import implicit\n",
    "model = implicit.nearest_neighbours.CosineRecommender()\n",
    "model.fit(user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_index 893, recommendations: [238, 239, 236, 234, 233, 229, 242, 231, 240, 230, 241, 235, 232, 243, 225]\n"
     ]
    }
   ],
   "source": [
    "random_user_index = np.random.choice(np.arange(start=0, stop=user_item.shape[1], step=1, dtype=np.uint32))\n",
    "recommended = model.recommend(random_user_index, user_item, 15)\n",
    "\n",
    "recommendations = []\n",
    "scores = []\n",
    "\n",
    "for item in recommended:\n",
    "    idx, score = item\n",
    "    recommendations.append(idx)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('user_index %d, recommendations: %s' % (random_user_index, recommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание: Item to Item\n",
    "\n",
    "Решите задачу c2c рекомендаций - вызовите метод `similar_items` для  *item_id=1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar_items:  [(1, 0.9999999999999999), (0, 0.9952980993292889), (7, 0.9952863079166219), (2, 0.9951768998816896), (5, 0.9947149078229031), (20271, 0.9905321642238799), (20273, 0.9889862279059467), (8, 0.9831939420781712), (20269, 0.9822039197684757), (20272, 0.9792772453750449)]\n"
     ]
    }
   ],
   "source": [
    "# -- ВАШ КОД ТУТ --\n",
    "item_id = 1\n",
    "related = model.similar_items(item_id)\n",
    "print(\"similar_items: \",related)\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: обучаем Implicit\n",
    "\n",
    "Почитайте документацию по модулю implicit.als.AlternatingLeastSquares. Обучите ALS-рекомендатель и воспользуйтесь методом recommend для построения рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|████████████████████████████████████████| 15.0/15 [00:03<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_index 357, recommendations: [15925, 16635, 16421, 15236, 18429, 14903, 15593, 22331, 14902, 24782, 14904, 14905, 14901, 14900, 13973]\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares()\n",
    "model.fit(user_item)\n",
    "random_user_index = np.random.choice(np.arange(start=0, stop=user_item.shape[1], step=1, dtype=np.uint32))\n",
    "recommended = model.recommend(random_user_index, user_item, 15)\n",
    "\n",
    "recommendations = []\n",
    "scores = []\n",
    "\n",
    "for item in recommended:\n",
    "    idx, score = item\n",
    "    recommendations.append(idx)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('user_index %d, recommendations: %s' % (random_user_index, recommendations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание на метрики\n",
    "\n",
    "Даны два вектора - истинная история пользователя и объекты, которые считает релеватными ваша модель\n",
    "\n",
    "Вычислите\n",
    "\n",
    "* precision\n",
    "* recall\n",
    "* precision@5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.1\n",
      "Precision:  0.5\n",
      "Precision@5:  0.012958509750701858\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_interactions = [47315, 30004, 36322,  8942, 30820,  6086,  9126,   332, 16289,\n",
    "       39106, 39335, 48506, 48654,  9234, 29935,  2678, 36202, 22636, 18007, 39328, 15414, 30016, 35601,\n",
    "    58409, 21313,   386, 16303, 4397, 19644, 51887, 21659, 36325, 53030,  7764, 50266, 58734, 53419, 24121,\n",
    "    50806, 36092,  8868, 28037, 36131, 13561, 16298, 27508, 41722, 30189, 46490,  2676, 43328, 781, 48397,\n",
    "    41369, 39324, 36381, 39635, 27710, 47837, 28525, 12024, 56604, 41664, 37387, 48507, 413, 33526, 20059,\n",
    "    49781, 56648, 16283, 50805, 34254, 39325, 59374, 22620,  8865, 27512, 13875, 30011,  7621,\n",
    "    10544, 28076, 29716, 30054, 20490, 29466, 16852, 39363, 34250, 7024, 33541,   263, 21267, 25690, 23020,\n",
    "    41368, 53414,  2681, 30201] \n",
    "\n",
    "user_recs = [\n",
    "    50820, 27781, 36131, 50812, 36092, 12024, 59155, 30042, 15414, 19882, 21659, 27849, 39328, 34240, 2681,\n",
    "    21267, 50126, 58560, 7764, 49781\n",
    "]\n",
    "\n",
    "# --- ВАШ КОД ТУТ ---\n",
    "k_total_num_views = len(user_interactions)\n",
    "n_total_num_recs = len(user_recs)\n",
    "m_viewed_recs = 0\n",
    "for el in user_recs:\n",
    "    if (el in user_interactions):\n",
    "        m_viewed_recs += 1\n",
    "\n",
    "recall = m_viewed_recs/k_total_num_views\n",
    "precision = m_viewed_recs/n_total_num_recs\n",
    "\n",
    "precision_5 = 0\n",
    "i = 1\n",
    "for el in user_interactions:\n",
    "    if (el in user_recs):\n",
    "        precision_5 += 1/i\n",
    "    i += 1\n",
    "precision_5 /= n_total_num_recs\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Precision@5: \", precision_5)\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
